[1,0]<stderr>:  0%|          | 0/100 [00:00<?, ?it/s][1,0]<stderr>:100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1815.39it/s][1,0]<stderr>:
[1,0]<stderr>:Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']
[1,0]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[1,0]<stderr>:/nobackup/users/gtangg12/anaconda3/envs/task_planning_babyai/lib/python3.8/site-packages/torch/utils/tensorboard/__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
[1,0]<stderr>:  if not hasattr(tensorboard, '__version__') or LooseVersion(tensorboard.__version__) < LooseVersion('1.15'):
[1,0]<stderr>:***** Running training *****
[1,0]<stderr>:  Num examples = 85
[1,0]<stderr>:  Num Epochs = 5
[1,0]<stderr>:  Instantaneous batch size per device = 4
[1,0]<stderr>:  Total train batch size (w. parallel, distributed & accumulation) = 32
[1,0]<stderr>:  Gradient Accumulation steps = 4
[1,0]<stderr>:  Total optimization steps = 10
[1,0]<stderr>:  0%|          | 0/10 [00:00<?, ?it/s][1,0]<stderr>:/nobackup/users/gtangg12/anaconda3/envs/task_planning_babyai/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
[1,0]<stderr>:  warnings.warn('Was asked to gather along dimension 0, but all '
[1,0]<stderr>:/nobackup/users/gtangg12/anaconda3/envs/task_planning_babyai/lib/python3.8/site-packages/torch/cuda/nccl.py:47: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
[1,0]<stderr>:  if not isinstance(inputs, collections.Container) or isinstance(inputs, torch.Tensor):
[1,0]<stderr>: 10%|â–ˆ         | 1/10 [00:04<00:40,  4.50s/it][1,0]<stderr>: 20%|â–ˆâ–ˆ        | 2/10 [00:06<00:22,  2.82s/it][1,0]<stderr>:                                              [1,0]<stderr>: 20%|â–ˆâ–ˆ        | 2/10 [00:07<00:22,  2.82s/it][1,0]<stderr>:***** Running Evaluation *****
[1,0]<stderr>:  Num examples = 15
[1,0]<stderr>:  Batch size = 8
[1,0]<stderr>:
[1,0]<stderr>:  0%|          | 0/2 [00:00<?, ?it/s][1,0]<stderr>:[A[1,0]<stderr>:
[1,0]<stderr>:100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.46it/s][1,0]<stderr>:[A[1,0]<stderr>:Trainer is attempting to log a value of "{0: 15}" of type <class 'dict'> for key "eval/preds_freq" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.
[1,0]<stderr>:Trainer is attempting to log a value of "{0: 1, 1: 1, 2: 13}" of type <class 'dict'> for key "eval/labels_freq" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.
[1,0]<stderr>:                                              [1,0]<stderr>:[1,0]<stderr>:
[1,0]<stderr>:                                             [1,0]<stderr>:[A[1,0]<stderr>: 20%|â–ˆâ–ˆ        | 2/10 [00:07<00:22,  2.82s/it][1,0]<stderr>:
[1,0]<stderr>:100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.46it/s][1,0]<stderr>:[A[1,0]<stderr>:
[1,0]<stderr>:                                             [A[1,0]<stderr>:Saving model checkpoint to /nobackup/users/gtangg12/task_planning_bayes_factor/checkpoints/babyai_gpt2_finetune_num_data/n100/checkpoint-2
[1,0]<stderr>:Configuration saved in /nobackup/users/gtangg12/task_planning_bayes_factor/checkpoints/babyai_gpt2_finetune_num_data/n100/checkpoint-2/config.json
[1,0]<stderr>:Model weights saved in /nobackup/users/gtangg12/task_planning_bayes_factor/checkpoints/babyai_gpt2_finetune_num_data/n100/checkpoint-2/pytorch_model.bin
[1,0]<stderr>:/nobackup/users/gtangg12/anaconda3/envs/task_planning_babyai/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
[1,0]<stderr>:  warnings.warn('Was asked to gather along dimension 0, but all '
[1,0]<stderr>: 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:11<00:26,  3.83s/it][1,0]<stderr>: 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:12<00:17,  2.95s/it][1,0]<stderr>:                                              [1,0]<stderr>: 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:13<00:17,  2.95s/it][1,0]<stderr>:***** Running Evaluation *****
[1,0]<stderr>:  Num examples = 15
[1,0]<stderr>:  Batch size = 8
[1,0]<stderr>:
[1,0]<stderr>:  0%|          | 0/2 [00:00<?, ?it/s][1,0]<stderr>:[A[1,0]<stderr>:
[1,0]<stderr>:100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.48it/s][1,0]<stderr>:[A[1,0]<stderr>:Trainer is attempting to log a value of "{0: 0, 1: 0, 2: 15}" of type <class 'dict'> for key "eval/preds_freq" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.
[1,0]<stderr>:Trainer is attempting to log a value of "{0: 1, 1: 1, 2: 13}" of type <class 'dict'> for key "eval/labels_freq" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.
[1,0]<stderr>:                                              [1,0]<stderr>:[1,0]<stderr>:
[1,0]<stderr>:                                             [1,0]<stderr>:[A[1,0]<stderr>: 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:14<00:17,  2.95s/it][1,0]<stderr>:
[1,0]<stderr>:100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.48it/s][1,0]<stderr>:[A[1,0]<stderr>:
[1,0]<stderr>:                                             [A[1,0]<stderr>:Saving model checkpoint to /nobackup/users/gtangg12/task_planning_bayes_factor/checkpoints/babyai_gpt2_finetune_num_data/n100/checkpoint-4
[1,0]<stderr>:Configuration saved in /nobackup/users/gtangg12/task_planning_bayes_factor/checkpoints/babyai_gpt2_finetune_num_data/n100/checkpoint-4/config.json
[1,0]<stderr>:Model weights saved in /nobackup/users/gtangg12/task_planning_bayes_factor/checkpoints/babyai_gpt2_finetune_num_data/n100/checkpoint-4/pytorch_model.bin
[1,0]<stderr>:/nobackup/users/gtangg12/anaconda3/envs/task_planning_babyai/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
[1,0]<stderr>:  warnings.warn('Was asked to gather along dimension 0, but all '
[1,0]<stderr>: 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:17<00:18,  3.62s/it][1,0]<stderr>: 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:19<00:11,  2.94s/it][1,0]<stderr>:                                              [1,0]<stderr>: 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:20<00:11,  2.94s/it][1,0]<stderr>:***** Running Evaluation *****
[1,0]<stderr>:  Num examples = 15
[1,0]<stderr>:  Batch size = 8
[1,0]<stderr>:
[1,0]<stderr>:  0%|          | 0/2 [00:00<?, ?it/s][1,0]<stderr>:[A[1,0]<stderr>:
[1,0]<stderr>:100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.45it/s][1,0]<stderr>:[A[1,0]<stderr>:Trainer is attempting to log a value of "{0: 0, 1: 0, 2: 15}" of type <class 'dict'> for key "eval/preds_freq" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.
[1,0]<stderr>:Trainer is attempting to log a value of "{0: 1, 1: 1, 2: 13}" of type <class 'dict'> for key "eval/labels_freq" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.
[1,0]<stderr>:                                              [1,0]<stderr>:[1,0]<stderr>:
[1,0]<stderr>:                                             [1,0]<stderr>:[A[1,0]<stderr>: 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:20<00:11,  2.94s/it][1,0]<stderr>:
[1,0]<stderr>:100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.45it/s][1,0]<stderr>:[A[1,0]<stderr>:
[1,0]<stderr>:                                             [A[1,0]<stderr>:Saving model checkpoint to /nobackup/users/gtangg12/task_planning_bayes_factor/checkpoints/babyai_gpt2_finetune_num_data/n100/checkpoint-6
[1,0]<stderr>:Configuration saved in /nobackup/users/gtangg12/task_planning_bayes_factor/checkpoints/babyai_gpt2_finetune_num_data/n100/checkpoint-6/config.json
[1,0]<stderr>:Model weights saved in /nobackup/users/gtangg12/task_planning_bayes_factor/checkpoints/babyai_gpt2_finetune_num_data/n100/checkpoint-6/pytorch_model.bin
[1,0]<stderr>:/nobackup/users/gtangg12/anaconda3/envs/task_planning_babyai/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
[1,0]<stderr>:  warnings.warn('Was asked to gather along dimension 0, but all '
[1,0]<stderr>: 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:24<00:10,  3.57s/it][1,0]<stderr>: 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:25<00:05,  2.94s/it][1,0]<stderr>:                                              [1,0]<stderr>: 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:26<00:05,  2.94s/it][1,0]<stderr>:***** Running Evaluation *****
[1,0]<stderr>:  Num examples = 15
[1,0]<stderr>:  Batch size = 8
[1,0]<stderr>:
[1,0]<stderr>:  0%|          | 0/2 [00:00<?, ?it/s][1,0]<stderr>:[A[1,0]<stderr>:
[1,0]<stderr>:100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.41it/s][1,0]<stderr>:[A[1,0]<stderr>:Trainer is attempting to log a value of "{0: 0, 1: 0, 2: 15}" of type <class 'dict'> for key "eval/preds_freq" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.
[1,0]<stderr>:Trainer is attempting to log a value of "{0: 1, 1: 1, 2: 13}" of type <class 'dict'> for key "eval/labels_freq" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.
[1,0]<stderr>:                                              [1,0]<stderr>:[1,0]<stderr>:
[1,0]<stderr>:                                             [1,0]<stderr>:[A[1,0]<stderr>: 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:27<00:05,  2.94s/it][1,0]<stderr>:
[1,0]<stderr>:100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.41it/s][1,0]<stderr>:[A[1,0]<stderr>:
[1,0]<stderr>:                                             [A[1,0]<stderr>:Saving model checkpoint to /nobackup/users/gtangg12/task_planning_bayes_factor/checkpoints/babyai_gpt2_finetune_num_data/n100/checkpoint-8
[1,0]<stderr>:Configuration saved in /nobackup/users/gtangg12/task_planning_bayes_factor/checkpoints/babyai_gpt2_finetune_num_data/n100/checkpoint-8/config.json
[1,0]<stderr>:Model weights saved in /nobackup/users/gtangg12/task_planning_bayes_factor/checkpoints/babyai_gpt2_finetune_num_data/n100/checkpoint-8/pytorch_model.bin
[1,0]<stderr>:/nobackup/users/gtangg12/anaconda3/envs/task_planning_babyai/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
[1,0]<stderr>:  warnings.warn('Was asked to gather along dimension 0, but all '
[1,0]<stderr>: 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:30<00:03,  3.55s/it][1,0]<stderr>:100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  2.95s/it][1,0]<stderr>:                                               [1,0]<stderr>:[1,0]<stderr>:100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  2.95s/it][1,0]<stderr>:***** Running Evaluation *****
[1,0]<stderr>:  Num examples = 15
[1,0]<stderr>:  Batch size = 8
[1,0]<stderr>:
[1,0]<stderr>:  0%|          | 0/2 [00:00<?, ?it/s][1,0]<stderr>:[A[1,0]<stderr>:
[1,0]<stderr>:100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.08it/s][1,0]<stderr>:[A[1,0]<stderr>:Trainer is attempting to log a value of "{0: 0, 1: 0, 2: 15}" of type <class 'dict'> for key "eval/preds_freq" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.
[1,0]<stderr>:Trainer is attempting to log a value of "{0: 1, 1: 1, 2: 13}" of type <class 'dict'> for key "eval/labels_freq" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.
[1,0]<stderr>:                                               [1,0]<stderr>:[1,0]<stderr>:
[1,0]<stderr>:                                             [1,0]<stderr>:[A[1,0]<stderr>:100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  2.95s/it][1,0]<stderr>:
[1,0]<stderr>:100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.08it/s][1,0]<stderr>:[A[1,0]<stderr>:
[1,0]<stderr>:                                             [A[1,0]<stderr>:Saving model checkpoint to /nobackup/users/gtangg12/task_planning_bayes_factor/checkpoints/babyai_gpt2_finetune_num_data/n100/checkpoint-10
[1,0]<stderr>:Configuration saved in /nobackup/users/gtangg12/task_planning_bayes_factor/checkpoints/babyai_gpt2_finetune_num_data/n100/checkpoint-10/config.json
[1,0]<stderr>:Model weights saved in /nobackup/users/gtangg12/task_planning_bayes_factor/checkpoints/babyai_gpt2_finetune_num_data/n100/checkpoint-10/pytorch_model.bin
[1,0]<stderr>:
[1,0]<stderr>:
[1,0]<stderr>:Training completed. Do not forget to share your model on huggingface.co/models =)
[1,0]<stderr>:
[1,0]<stderr>:
[1,0]<stderr>:                                               [1,0]<stderr>:[1,0]<stderr>:100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:34<00:00,  2.95s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:34<00:00,  3.42s/it]
